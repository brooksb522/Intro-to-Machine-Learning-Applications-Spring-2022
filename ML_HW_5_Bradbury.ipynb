{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b8e4c05",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "90e3822dcaa14245c16ec3276dfa01b3",
          "grade": false,
          "grade_id": "instructions",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1b8e4c05"
      },
      "source": [
        "#### Homework-5\n",
        "##### Total number of points: 70\n",
        "#### Due date: March 4th, 2022\n",
        "\n",
        "Before you submit this homework, make sure everything runs as expected. First, restart the kernel (in the menu, select Kernel → Restart) and then run all cells (in the menubar, select Cell → Run All). You can discuss with others regarding the homework but all work must be your own.\n",
        "\n",
        "This homework will test your knowledge on basics of Python. The Python notebooks shared will be helpful to solve these problems. \n",
        "\n",
        "Steps to evaluate your solutions:\n",
        "\n",
        "Step-1: Ensure you have installed Anaconda (Windows: https://docs.anaconda.com/anaconda/install/windows/ ; Mac:https://docs.anaconda.com/anaconda/install/mac-os/ ; Linux: https://docs.anaconda.com/anaconda/install/linux/)\n",
        "\n",
        "Step-2: Open the Jupyter Notebook by first launching the anaconda software console\n",
        "\n",
        "Step-3: Open the .ipynb file and write your solutions at the appropriate location \"# YOUR CODE HERE\"\n",
        "\n",
        "Step-4: You can restart the kernel and click run all (in the menubar, select Cell → Run All) on the center-right on the top of this window.\n",
        "\n",
        "Step-5: Now go to \"File\" then click on \"Download as\" then click on \"Notebook (.ipynb)\" Please DO NOT change the file name and just keep it as .ipynb file format\n",
        "\n",
        "Step-6: Go to lms.rpi.edu and upload your homework at the appropriate link to submit this homework.\n",
        "\n",
        "#### Please note that for any question in this assignment you will receive points ONLY if your solution passes all the test cases including hidden testcases as well. So please make sure you try to think all possible scenarios before submitting your answers.  \n",
        "- Note that hidden tests are present to ensure you are not hardcoding. \n",
        "- If caught cheating: \n",
        "    - you will receive a score of 0 for the 1st violation. \n",
        "    - for repeated incidents, you will receive an automatic 'F' grade and will be reported to the dean of Lally School of Management. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58506fd2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "53cbe565f27348589091978e4b849da6",
          "grade": false,
          "grade_id": "q1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "58506fd2"
      },
      "source": [
        "#### Q1 [10 points]. Please make sure this is correct as the following questions are dependent on this. \n",
        "- Load the boston data from sklearn library (library is already provided here below). \n",
        "- The given target labels (boston.target) are continuous -- convert them into discrete values -- 1 and 2 using this approach.\n",
        "- all the values (v1) in boston.target should become 1 if v1>=5 and v1<23; \n",
        "- all the values (v2) in boston.target should become 2 if v2>=23 and v2<51;\n",
        "- Note that these new/transformed labels 1 and 2 are **integers**\n",
        "- Please save these new discrete values in the variable `y_true`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6285a9",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4b92b319cb8b69abfa97b3cdc43ac237",
          "grade": false,
          "grade_id": "q1-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba6285a9",
        "outputId": "02b1c0f3-bfd7-4c53-f709-6cc9dee05970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "#Load the dataset and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "# YOUR CODE HERE\n",
        "\n",
        "test1 = np.where((boston.target >= 5) & (boston.target < 23), 1, boston.target)\n",
        "test2 = np.where((test1 >= 23) & (test1 < 51), 2, test1)\n",
        "\n",
        "y_true = test2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3f11f8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f65d21e17c5013b0de56075bb218a73f",
          "grade": true,
          "grade_id": "1",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ac3f11f8"
      },
      "outputs": [],
      "source": [
        "#[10 points] Test cell-1 \n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert set(y_true)=={1,2}\n",
        "assert (list(y_true)).count(1)==312\n",
        "assert (list(y_true)).count(0)==0\n",
        "assert (list(y_true)).count(2)==194"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fea31d1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bc05c36c4783af238926586ee83dff3a",
          "grade": false,
          "grade_id": "q2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5fea31d1"
      },
      "source": [
        "#### Q2 [5 points]. Now split the data into training (80%) and testing data (20%) by using these variable names \n",
        "- `X_train`: Training feature columns\n",
        "- `X_test`: Testing feature columns\n",
        "- `y_train`: Training labels\n",
        "- `y_test`: Testing labels\n",
        "- with only parameters df, y_true and 'test_size'. `df` and `y_true` are initialized in the previous question. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec16ae9d",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "296509c5901df0967f2639328a908f79",
          "grade": false,
          "grade_id": "q2-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ec16ae9d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = pd.DataFrame()\n",
        "y_train = pd.DataFrame()\n",
        "X_test = pd.DataFrame()\n",
        "y_test = pd.DataFrame()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y_true, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc94621",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d26c961278bcacc042ebdd3e6e14b2ae",
          "grade": true,
          "grade_id": "2",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2dc94621"
      },
      "outputs": [],
      "source": [
        "#[10 points] Test cell-2\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(X_train)==404\n",
        "assert len(y_test)==102\n",
        "assert len(y_train)==404\n",
        "assert len(X_test)==102"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f080f1b9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "87a9a7c587d3852df72046a214dafda6",
          "grade": false,
          "grade_id": "q3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f080f1b9"
      },
      "source": [
        "#### Q3 [5 points]. Use the `df` in Q1 to perform these operations: \n",
        "<ol>\n",
        "<li> Standardize the data using StandardScaler() function from sklearn library</li>\n",
        "<li> Compute principal components using the fit_transform operation on the original dataset </li>\n",
        "<li> Now find the number of principal components `n_components` that will retain not more than 75% of the information present in the original dataset </li>\n",
        "</ol>\n",
        "\n",
        "- Make sure you declare appropriate Python packages which are not provided by default\n",
        "- Hint -- Use explained_variance_ratio_.cumsum() function we discussed in the class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a4c265",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b1b968f5bf118d91cdc4e042a8c4c28b",
          "grade": false,
          "grade_id": "q3-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "a4a4c265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685d0076-b9d8-4fcc-bc3c-8d28644c9559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of components under threshhold is: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Load the dataset and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled = scaler.fit_transform(df)\n",
        "df_scaled = pd.DataFrame(scaled, columns=list(df.columns))\n",
        "df_scaled.head()\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(df_scaled)\n",
        "components = pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "n_components=0\n",
        "\n",
        "for i in range(len(components)):\n",
        "  if components[i] < 0.75:\n",
        "    n_components = n_components + 1\n",
        "\n",
        "print('The number of components under threshhold is:',n_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7980f1c4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "36be9ea7134f87dacae078d7a6cbf905",
          "grade": true,
          "grade_id": "3",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7980f1c4"
      },
      "outputs": [],
      "source": [
        "#[2.5 points] Test cell-3\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert n_components!=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "954bda19",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e4756a009809e532cc15d679c8ed9cba",
          "grade": true,
          "grade_id": "4",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "954bda19"
      },
      "outputs": [],
      "source": [
        "#[2.5 points] Test cell-4 Hidden tests\n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b47c63a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2242180edc1f38a408cb38f882104f2a",
          "grade": false,
          "grade_id": "q4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7b47c63a"
      },
      "source": [
        "#### Q4 [10 points]. Given 3 different series `S1`, `S2`, `S3`\n",
        "<ol>\n",
        "<li>First, using these 3 series build a dataframe 'df' where the column name associated with each series is same as the series name such as `S1`, `S2` or `S3`.</li>\n",
        "<li>Please note that 'df' doesn't contain any column representing class labels. </li>\n",
        "<li>If you perform PCA operation, how many maximum principal components will you obtain?</li>\n",
        "<li>No need to do pca but assign the value to variable `max_pcs`</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2308ca",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e3b4bac3ff8122fe6b11572be6df21ff",
          "grade": false,
          "grade_id": "q4-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ae2308ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65539168-5714-4443-bdaa-f396c737ded4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "S1 = pd.Series([1,2,3,4]) #corresponding column name should be 'S1'\n",
        "S2 = pd.Series([10, 20, 30, 40]) #corresponding column name should be 'S2'\n",
        "S3 = pd.Series([2,4,6,8]) #corresponding column name should be 'S3'\n",
        "\n",
        "df = pd.DataFrame({'S1': S1, 'S2': S2, 'S3': S3})\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled = scaler.fit_transform(df)\n",
        "df_scaled = pd.DataFrame(scaled, columns=list(df.columns))\n",
        "df_scaled.head()\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(df_scaled)\n",
        "\n",
        "pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "#max_pcs=0 #Update this variable after building the dataframe #No need to perform PCA \n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6c71e7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "790ee5043eae8065fcb581d54c843bd2",
          "grade": true,
          "grade_id": "5",
          "locked": true,
          "points": 7.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6a6c71e7"
      },
      "outputs": [],
      "source": [
        "#[7.5 points] Test cell-5\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert set(df.columns)=={'S1', 'S2', 'S3'}\n",
        "assert df['S3'].mean()==5.0\n",
        "assert df['S1'].mean()==2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ff071a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "783cf9f24922a0345426cb40561e1470",
          "grade": true,
          "grade_id": "6",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "10ff071a"
      },
      "outputs": [],
      "source": [
        "#[2.5 points] Test cell-6 Hidden tests\n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0428eb49",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d6fd8f4698fd5fc77a0331a1e2eb9d68",
          "grade": false,
          "grade_id": "q5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0428eb49"
      },
      "source": [
        "#### Q5 [10 points]. Linear regression: Use the boston dataset loaded into the dataframe `df`\n",
        "##### 1. **Without using train_test_split() function** use the 1st 400 rows (i.e., index 0,1,....399) where, \n",
        "- the corresponding features are loaded as `train_X` and corresponding labels as `train_y`\n",
        "- Use the remaining rows as testing data -- `test_X` and `test_y` for the features and labels respectively.\n",
        "- Fit a linear regression line using the training data; then use it to predict labels for testing data as shown in the lecture notebook. Please use the default parameters when calling the linear regression function. \n",
        "\n",
        "##### 2. Measure the mean-squared error (MSE) 'mse_split1' using the predicted labels with testing labels.\n",
        "- Round the `mse_split1` to 2 values after the decimal point. \n",
        "- Hint -- Check out the libraries given to guess which functions to use to compute MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac2a6e6",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a55cbe99214cd58689c82add6c0d3031",
          "grade": false,
          "grade_id": "q5-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8ac2a6e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9efe8e81-eb29-4284-edf4-92b7b3ddafae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37.89\n",
            "400\n",
            "106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load the dataset without class labels and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "df\n",
        "\n",
        "# class label -- boston.target\n",
        "\n",
        "train_X = pd.DataFrame(df[:400])\n",
        "train_y = pd.DataFrame(boston.target[:400])\n",
        "test_X = pd.DataFrame(df[400:])\n",
        "test_y = pd.DataFrame(boston.target[400:])\n",
        "\n",
        "lm = LinearRegression().fit(train_X, train_y)\n",
        "y_pred = lm.predict(test_X)\n",
        "mse_split1 = round(mean_squared_error(test_y, y_pred),2)\n",
        "print(mse_split1)\n",
        "print(len(train_y))\n",
        "print(len(test_X))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fad5071",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "403cd337a28564ec895ad5e9060e960b",
          "grade": true,
          "grade_id": "7",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4fad5071"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-7\n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(train_X)==400\n",
        "assert len(test_y)==106\n",
        "assert mse_split1==37.89\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce17d97",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4f00f33311d6149c87e89c7c1aeb17d9",
          "grade": true,
          "grade_id": "8",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8ce17d97"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-8 Hidden tests \n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0599097",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4f6c51f955eb2fb85acd22ad37058b43",
          "grade": false,
          "grade_id": "q6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "b0599097"
      },
      "source": [
        "#### Q6 [10 points]. Repeat the above exercise (Q5) but with different training and testing splits using the `boston` dataset.\n",
        "\n",
        "###### 1. **Without using train_test_split() function** use the last 253 rows (i.e., index 253,254,....505) where, \n",
        "- the corresponding features are loaded as `train_X` and corresponding labels as `train_y`\n",
        "- Use the remaining rows as testing data -- `test_X` and `test_y` for the features and labels respectively.\n",
        "- Fit a linear regression line using the training data; then use it for predicting testing data as shown in the lecture notebook. Please use the default parameters when calling the linear regression function. \n",
        "\n",
        "###### 2. Measure the mean-squared error (MSE) 'mse_split2' using the predicted labels with testing labels.\n",
        "- Round the `mse_split2` to 2 values after the decimal point. \n",
        "- Hint -- Check out the libraries given to guess which functions to use to compute MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcd008d",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9a83bf208a4218d9752e7246175da5d1",
          "grade": false,
          "grade_id": "q6-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "2dcd008d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86164bf5-0fff-45d5-beca-40ac335cf2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27.22\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Load the dataset without class labels and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "#class label -- boston.target\n",
        "\n",
        "train_X = pd.DataFrame(df[253:])\n",
        "train_y = pd.DataFrame(boston.target[253:])\n",
        "test_X = pd.DataFrame(df[:253])\n",
        "test_y = pd.DataFrame(boston.target[:253])\n",
        "\n",
        "lm = LinearRegression().fit(train_X, train_y)\n",
        "y_pred = lm.predict(test_X)\n",
        "mse_split2 = round(mean_squared_error(test_y, y_pred),2)\n",
        "print(mse_split2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3240fe",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4626e3095cf83a78813c5922565e007f",
          "grade": true,
          "grade_id": "9",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1c3240fe"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-9  \n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(train_X)==253\n",
        "assert len(test_y)==253\n",
        "assert mse_split2==27.22\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e3b1f6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d95c8620375079b17486fdba7251441f",
          "grade": true,
          "grade_id": "10",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "29e3b1f6"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-10 Hidden tests \n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899af61b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8b4ce2be123f6b5626126fa733c13c05",
          "grade": false,
          "grade_id": "q7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "899af61b"
      },
      "source": [
        "#### Q7 [15 points]. We have loaded the boston dataset -- use the dataframe `df` to do: \n",
        "###### 1. Convert the target labels -- boston.target from continuous to discrete values -- 1, 2 and 3 using this approach.\n",
        "- all the values (lets say, each value is represented by v1) in boston.target should become 1 if v1>=5 and v1<20; \n",
        "- all the values (lets say, each value is represented by v2) in boston.target should become 2 if v2>=20 and v2<35;\n",
        "- all the values (lets say, each value is represented by v3) in boston.target should become 3 if v3>=35 and v3<51;\n",
        "\n",
        "###### 2. **Without using train_test_split()** use the 1st 400 rows (i.e., index 0,1,....399) where, \n",
        "- the corresponding features are loaded as `train_X` and corresponding labels as `train_y`\n",
        "- Use the rest of the rows for testing data -- `test_X` and `test_y` for the features and groundtruth labels respectively.\n",
        "- Fit a logistic regression with solver=`newton-cg`, C=1e5, multi_class=`multinomial`; then use it for predicting testing data as shown in the lecture notebook. If you encounter this warning, please ignore it -- \"ConvergenceWarning: newton-cg failed to converge.\" \n",
        "\n",
        "###### 3. Measure the accuracy `acc_split3` using the predicted labels with groundtruth test labels `test_y`.\n",
        "- Round the `acc_split3` to 2 values after the decimal point. \n",
        "- Hint -- Check out the libraries given to guess which functions to use to compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2c5c66",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6513ba1528ab968a0fc40633b504689e",
          "grade": false,
          "grade_id": "q7-sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5b2c5c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9f42b0-3be1-4e45-f72d-9c943227a001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Load the dataset without class labels and save it as a dataframe\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "#class label -- boston.target\n",
        "\n",
        "phase1 = np.where((boston.target >= 5) & (boston.target < 20), 1, boston.target)\n",
        "phase2 = np.where((phase1 >= 20) & (phase1 < 35), 2, phase1)\n",
        "phase3 = np.where((phase2 >= 35) & (phase2 < 51), 3, phase2)\n",
        "targetlabels = phase3\n",
        "\n",
        "train_X = pd.DataFrame(df[:400])\n",
        "train_y = pd.DataFrame(targetlabels[:400])\n",
        "test_X = pd.DataFrame(df[400:])\n",
        "test_y = pd.DataFrame(targetlabels[400:])\n",
        "\n",
        "logR = LogisticRegression(solver='newton-cg',C=1e5,multi_class='multinomial').fit(train_X, train_y)\n",
        "y_pred1 = logR.predict(df)\n",
        "acc_split3 = round(accuracy_score(test_y, y_pred1[400:]),2)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16101e15",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c4dcc5d1e9a72f9ca3ed89fbdb539d3f",
          "grade": true,
          "grade_id": "11",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "16101e15"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-11 \n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert len(train_X)==400\n",
        "assert set(targetlabels)=={1,2,3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319e41c2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fc61d37948ddd786bfc042a91a3a1873",
          "grade": true,
          "grade_id": "12",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "319e41c2"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-12  \n",
        "#DO NOT MODIFY/DELETE THIS CELL \n",
        "assert acc_split3==0.74\n",
        "assert len(test_y)==106"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6318191b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "213f5cd94bf5db600d069119334c27a2",
          "grade": true,
          "grade_id": "13",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6318191b"
      },
      "outputs": [],
      "source": [
        "#[5 points] Test cell-13 Hidden tests \n",
        "#DO NOT MODIFY/DELETE THIS CELL "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}